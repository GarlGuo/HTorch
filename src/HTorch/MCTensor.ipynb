{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\23128\\anaconda3\\envs\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "import math\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from HTorch.MCTensor.MCOpBasics import _Renormalize, _Simple_renormalize_old, _Grow_ExpN, _AddMCN,  _ScalingN,\\\n",
    "    _DivMCN, _MultMCN, _exp, _pow, _square, _sinh, _cosh, _tanh, _log, _exp, _sqrt, \\\n",
    "    _softmax, _log_softmax, _cross_entropy, _mse_loss, _layer_norm, _atanh, _log1p_standard, \\\n",
    "    _clamp, _norm, _sum, _mean\n",
    "from HTorch.MCTensor.MCOpMatrix import _Dot_MCN, _Dot_MCN_M, _MV_MC_T, _MV_T_MC, _MV_MC_M_M, _MM_MC_T, _MM_T_MC, _MM_MC_MC, \\\n",
    "    _BMM_MC_T, _BMM_T_MC, _BMM_MC_MC, _4DMM_T_MC, _4DMM_MC_T, _4DMM_MC_MC\n",
    "from typing import Union, List\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HANDLED_FUNCTIONS = {}\n",
    "\n",
    "def implements(torch_function):\n",
    "    \"\"\"Register a torch function override for MCTensor\"\"\"\n",
    "    @functools.wraps(torch_function)\n",
    "    def decorator(func):\n",
    "        HANDLED_FUNCTIONS[torch_function] = func\n",
    "        return func\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTensor(Tensor):\n",
    "    @staticmethod \n",
    "    def __new__(cls, *args,  nc=1, **kwargs): \n",
    "        ret = super().__new__(cls, *args, **kwargs)\n",
    "        ret._nc = nc\n",
    "        ret.res = torch.zeros(ret.size() + (nc-1,), dtype=ret.dtype, device=ret.device)\n",
    "        return ret\n",
    "\n",
    "    def __init__(self, *args,  nc=1, **kwargs):\n",
    "        self._nc = nc\n",
    "        self.res = torch.zeros(self.size() + (nc-1,), dtype=self.dtype, device=self.device)\n",
    "        \n",
    "    @staticmethod\n",
    "    def wrap_tensor_to_mctensor(tensor: Tensor) -> MCTensor:\n",
    "        # involves copying\n",
    "        ret = MCTensor(tensor[..., 0], nc=tensor.size(-1))\n",
    "        ret.res.data.copy_(tensor[..., 1:].data)\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def wrap_tensor_and_res_to_mctensor(val: Tensor, res: Tensor) -> MCTensor:\n",
    "        # involves copying\n",
    "        ret = MCTensor(val, nc=res.shape[-1] + 1)\n",
    "        ret.res.data.copy_(res.data)\n",
    "        return ret\n",
    "\n",
    "    @property\n",
    "    def tensor(self):\n",
    "        return torch.cat([self.as_subclass(Tensor).view(*self.shape, 1), self.res], -1)\n",
    "\n",
    "    @property\n",
    "    def T(self):\n",
    "        return torch.transpose(self, 0, 1)\n",
    "    \n",
    "    @property\n",
    "    def nc(self):\n",
    "        return self._nc\n",
    "        \n",
    "    def normalize_(self, simple=False):\n",
    "        if simple:\n",
    "            normalized_self = _Simple_renormalize_old(self.tensor, self.nc)\n",
    "        else:\n",
    "            normalized_self = _Renormalize(self.tensor, self.nc)\n",
    "        self.data.copy_(normalized_self[..., 0].data)\n",
    "        self.res.data.copy_(normalized_self[..., 1:].data)\n",
    "    \n",
    "    def __repr__(self, *args, **kwargs):\n",
    "        return \"{}, nc={}\".format(super().__repr__(), self.nc)\n",
    "\n",
    "    def __add__(self, other) -> MCTensor:\n",
    "        ''' add self with other'''\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__add__(other)\n",
    "        else:\n",
    "            obj = torch.add(self, other)\n",
    "        return obj\n",
    "\n",
    "    def add(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def add_(self, other):\n",
    "        return self.copy_(self + other)\n",
    "\n",
    "    def __radd__(self, other) -> MCTensor:\n",
    "        ''' add self with other'''\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__radd__(other)\n",
    "        else:\n",
    "            obj = torch.add(self, other)\n",
    "        return obj\n",
    "    \n",
    "    def __sub__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__sub__(other)\n",
    "        else:\n",
    "            obj = torch.add(self, -other)\n",
    "        return obj\n",
    "\n",
    "    def sub(self, other):\n",
    "        return self - other\n",
    "\n",
    "    def sub_(self, other):\n",
    "        return self.copy_(self - other)\n",
    "\n",
    "    def __rsub__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__rsub__(other)\n",
    "        else:\n",
    "            obj = torch.add(other, -self)\n",
    "        return obj\n",
    "\n",
    "    def __mul__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__mul__(other)\n",
    "        else:\n",
    "            obj = torch.mul(self, other)\n",
    "        return obj\n",
    "\n",
    "    def mul(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def mul_(self, other):\n",
    "        return self.copy_(self * other)\n",
    "\n",
    "    def __rmul__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__rmul__(other)\n",
    "        else:\n",
    "            obj = torch.mul(other, self)\n",
    "        return obj\n",
    "    \n",
    "    def __truediv__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__truediv__(other)\n",
    "        else:\n",
    "            obj = torch.div(self, other)\n",
    "        return obj\n",
    "\n",
    "    def div(self, other):\n",
    "        return self / other\n",
    "\n",
    "    def div_(self, other):\n",
    "        return self.copy_(self / other)\n",
    "\n",
    "    def __rtruediv__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__rtruediv__(other)\n",
    "        else:\n",
    "            obj = torch.div(other, self)\n",
    "        return obj\n",
    "\n",
    "    def mv(self, other):\n",
    "        return torch.mv(self, other)\n",
    "\n",
    "    def mm(self, other):\n",
    "        return torch.mm(self, other)\n",
    "\n",
    "    def bmm(self, other):\n",
    "        return torch.bmm(self, other)\n",
    "\n",
    "    def matmul(self, other):\n",
    "        return torch.matmul(self, other)\n",
    "\n",
    "    def __matmul__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__matmul__(other)\n",
    "        else:\n",
    "            obj = torch.matmul(self, other)\n",
    "        return obj\n",
    "\n",
    "    def __rmatmul__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__rmatmul__(other)\n",
    "        else:\n",
    "            obj = torch.matmul(other, self)\n",
    "        return obj\n",
    "\n",
    "    def __pow__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__pow__(other)\n",
    "        else:\n",
    "            obj = torch.pow(self, other)\n",
    "        return obj\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, tuple) and Ellipsis in key:\n",
    "            key = key + (slice(None, None, None),) # case of [..., v]\n",
    "        val = self.tensor[key]\n",
    "        return MCTensor.wrap_tensor_to_mctensor(val)\n",
    "\n",
    "    def __setitem__(self, key, value: Union[MCTensor, Tensor, int, float]):\n",
    "        if isinstance(key, tuple) and Ellipsis in key:\n",
    "            res_key = (*key, slice(None, None, None))\n",
    "        else:\n",
    "            res_key = key\n",
    "        if isinstance(value, MCTensor):\n",
    "            super().__setitem__(key, value.as_subclass(Tensor))\n",
    "            self.res[res_key].data.copy_(value.res.data)\n",
    "        else:\n",
    "            super().__setitem__(key, value)\n",
    "            self.res[res_key] = 0\n",
    "\n",
    "    def dot(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            if isinstance(other, MCTensor):\n",
    "                other = other.as_subclass(Tensor)\n",
    "            obj = super().as_subclass(Tensor).dot(other)\n",
    "        else:\n",
    "            obj = torch.dot(self, other)\n",
    "        return obj\n",
    "    \n",
    "    def abs(self) -> MCTensor:\n",
    "        return torch.abs(self)\n",
    "    \n",
    "    def sum(self, dim=None, keepdim=False, **kw) -> MCTensor:\n",
    "        return torch.sum(self, dim=dim, keepdim=keepdim)\n",
    "    \n",
    "    def mean(self, dim=None, keepdim=False, **kw) -> MCTensor:\n",
    "        return torch.mean(self, dim=dim, keepdim=keepdim)\n",
    "\n",
    "    def norm(self, dim=None, keepdim=False, p=2, **kw) -> MCTensor:\n",
    "        return torch.norm(self, dim=dim, keepdim=keepdim, p=p)\n",
    "\n",
    "    def exp(self) -> MCTensor:\n",
    "        return torch.exp(self)\n",
    "\n",
    "    def exp_(self) -> MCTensor:\n",
    "        return self.copy_(torch.exp(self))\n",
    "\n",
    "    def log(self) -> MCTensor:\n",
    "        return torch.log(self)\n",
    "    \n",
    "    def log_(self) -> MCTensor:\n",
    "        return self.copy_(torch.log(self))\n",
    "\n",
    "    def square(self) -> MCTensor:\n",
    "        return torch.square(self)\n",
    "\n",
    "    def square_(self) -> MCTensor:\n",
    "        return self.copy_(torch.square(self))\n",
    "\n",
    "    def sqrt(self) -> MCTensor:\n",
    "        return torch.sqrt(self)\n",
    "\n",
    "    def sqrt_(self) -> MCTensor:\n",
    "        return self.copy_(torch.sqrt(self))\n",
    "\n",
    "    def sinh(self) -> MCTensor:\n",
    "        return torch.sinh(self)\n",
    "\n",
    "    def sinh_(self) -> MCTensor:\n",
    "        return self.copy_(torch.sinh(self))\n",
    "\n",
    "    def cosh(self) -> MCTensor:\n",
    "        return torch.cosh(self)\n",
    "    \n",
    "    def cosh_(self) -> MCTensor:\n",
    "        return self.copy_(torch.cosh(self))\n",
    "    \n",
    "    def tanh(self) -> MCTensor:\n",
    "        return torch.tanh(self)\n",
    "\n",
    "    def tanh_(self) -> MCTensor:\n",
    "        return self.copy_(torch.tanh(self))\n",
    "\n",
    "    def atanh(self) -> MCTensor:\n",
    "        return torch.atanh(self)\n",
    "\n",
    "    def atanh_(self) -> MCTensor:\n",
    "        return self.copy_(torch.atanh(self))\n",
    "    \n",
    "    def clamp_min(self, min=None) -> MCTensor:\n",
    "        return torch.clamp_min(self, min=min)\n",
    "\n",
    "    def clamp_min_(self, min=None) -> MCTensor:\n",
    "        return self.copy_(torch.clamp_min(self, min=min))\n",
    "\n",
    "    def clamp_max(self, max=None) -> MCTensor:\n",
    "        return torch.clamp_max(self, max=max)\n",
    "\n",
    "    def clamp_max_(self, max=None) -> MCTensor:\n",
    "        return self.copy_(torch.clamp_max(self, max=max))\n",
    "\n",
    "    def clone(self) -> MCTensor:\n",
    "        return torch.clone(self)\n",
    "\n",
    "    def unsqueeze(self, *args, **kwargs) -> MCTensor:\n",
    "        return torch.unsqueeze(self, *args, **kwargs)\n",
    "    \n",
    "    def squeeze(self, *args, **kwargs) -> MCTensor:\n",
    "        return torch.squeeze(self, *args, **kwargs)\n",
    "\n",
    "    def reshape(self, *shape) -> MCTensor:\n",
    "        return torch.reshape(self, shape)\n",
    "\n",
    "    def transpose(self, dim0, dim1) -> MCTensor:\n",
    "        return torch.transpose(self, dim0, dim1)\n",
    "\n",
    "    def narrow(self, dim, start, length) -> MCTensor:\n",
    "        if dim < 0:\n",
    "            dim = dim + self.dim()\n",
    "        return super().narrow(dim, start, length)\n",
    "    \n",
    "    def index_select(self, dim, index) -> MCTensor:\n",
    "        if dim < 0:\n",
    "            dim = dim + self.dim()\n",
    "        return super().index_select(dim, index)\n",
    "\n",
    "    def copy_(self, other: Union[MCTensor, Tensor]):\n",
    "        if type(other) == Tensor:\n",
    "            super().copy_(other)\n",
    "        elif isinstance(other, MCTensor):\n",
    "            super().copy_(other.as_subclass(Tensor))\n",
    "            self.res.data.copy_(other.res.data)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_args(args):\n",
    "        new_args = []\n",
    "        for arg in args:\n",
    "            if isinstance(arg, list):\n",
    "                # Recursively apply the function to each element of the list\n",
    "                new_arg = MCTensor.replace_args(arg)\n",
    "            elif isinstance(arg, MCTensor):\n",
    "                # Replace MCTensor with its `res` attribute\n",
    "                new_arg = arg.res\n",
    "            else:\n",
    "                # For other types of objects, just use the original object\n",
    "                new_arg = arg\n",
    "            new_args.append(new_arg)\n",
    "        return tuple(new_args)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        # inherence nc\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        if func in HANDLED_FUNCTIONS: # torch.Tensor with MCTensor contents\n",
    "            ret = HANDLED_FUNCTIONS[func](*args, **kwargs)\n",
    "            if isinstance(ret, Tensor) and not isinstance(ret, MCTensor):\n",
    "                return cls.wrap_tensor_to_mctensor(ret)\n",
    "        else: # pytorch functions handle main and res component separately\n",
    "            ret = super().__torch_function__(func, types, args, kwargs)\n",
    "            if isinstance(ret, MCTensor) and not hasattr(ret, '_nc'):\n",
    "                new_args = cls.replace_args(args)\n",
    "                ret.res = super().__torch_function__(func, types, new_args, kwargs).as_subclass(Tensor)\n",
    "                ret._nc = ret.res.size(-1) + 1\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@implements(torch.add)\n",
    "def add(input: Union[MCTensor, int, float], other: Union[MCTensor, int, float], alpha=1) -> MCTensor:\n",
    "    if type(input) == int or type(input) == float:\n",
    "        input = torch.tensor(input, dtype=other.dtype,\n",
    "                             device=other.device)\n",
    "    if type(other) == int or type(other) == float:\n",
    "        other = torch.tensor(other, device=input.device,\n",
    "                             dtype=input.dtype)\n",
    "    if alpha != 1:\n",
    "        other = alpha * other  # should check for MCTensor multiplication\n",
    "    if isinstance(input, MCTensor) and isinstance(other, MCTensor):\n",
    "        return _AddMCN(input.tensor, other.tensor, simple=False)\n",
    "    elif isinstance(input, MCTensor):\n",
    "        x = input  # the MCTensor\n",
    "        value = other\n",
    "    else:\n",
    "        x = other  # the MCTensor\n",
    "        value = input\n",
    "    x_tensor = x.tensor\n",
    "    if x_tensor.dim() == 1:\n",
    "        return _Grow_ExpN(x_tensor.unsqueeze(0), value)[0]\n",
    "    else:\n",
    "        return _Grow_ExpN(x_tensor, value)\n",
    "\n",
    "@implements(torch.mul)\n",
    "def mul(input: Union[MCTensor, int, float], other: Union[MCTensor, int, float]) -> MCTensor:\n",
    "    if type(input) == int or type(input) == float:\n",
    "        input = torch.tensor(input, dtype=other.dtype,\n",
    "                             device=other.device)\n",
    "    if type(other) == int or type(other) == float:\n",
    "        other = torch.tensor(other, device=input.device,\n",
    "                             dtype=input.dtype)\n",
    "    normalize_case = 0 #case 0, not renormalize, explore carefully\n",
    "    if isinstance(input, MCTensor) and isinstance(other, MCTensor):\n",
    "        return _MultMCN(input.tensor, other.tensor, case=normalize_case)\n",
    "    elif isinstance(input, MCTensor):\n",
    "        x = input  # the MCTensor\n",
    "        value = other\n",
    "    else:\n",
    "        x = other  # the MCTensor\n",
    "        value = input\n",
    "    return _ScalingN(x.tensor, value)\n",
    "\n",
    "@implements(torch.div)\n",
    "def div(x: Union[MCTensor, int, float], y: Union[MCTensor, int, float]) -> MCTensor:\n",
    "    if type(x) == int or type(x) == float:\n",
    "        x = torch.tensor(x, device=y.tensor.device, dtype=y.tensor.dtype)\n",
    "    if type(y) == int or type(y) == float:\n",
    "        y = torch.tensor(y, device=x.tensor.device, dtype=x.tensor.dtype)\n",
    "    normalize_case = 2 #case 2, renormalize, explore carefully\n",
    "    if isinstance(x, MCTensor) and type(y) == Tensor:\n",
    "        nc = x.nc\n",
    "        y_tensor = torch.zeros(y.size() + (nc,), device=y.device, dtype=y.dtype)\n",
    "        y_tensor[..., 0] = y\n",
    "        result = _DivMCN(x.tensor, y_tensor, case=normalize_case)\n",
    "    elif type(x) == Tensor and isinstance(y, MCTensor):\n",
    "        nc = y.nc\n",
    "        x_tensor = torch.zeros(x.size() + (nc,), device=x.device, dtype=x.dtype)\n",
    "        x_tensor[..., 0] = x\n",
    "        result = _DivMCN(x_tensor, y.tensor, case=normalize_case)\n",
    "    elif isinstance(x, MCTensor) and isinstance(y, MCTensor):\n",
    "        result = _DivMCN(x.tensor, y.tensor, case=normalize_case)\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    return result\n",
    "\n",
    "@implements(torch.rand_like)\n",
    "def rand_like(input: MCTensor, requires_grad=False, device=None, dtype=None) -> MCTensor:\n",
    "    val = torch.randn_like(input.as_subclass(Tensor), requires_grad=requires_grad, device=device, dtype=dtype)\n",
    "    return MCTensor(val, nc=input.nc)\n",
    "\n",
    "@implements(torch.zeros_like)\n",
    "def zeros_like(input: MCTensor, requires_grad=False, device=None, dtype=None) -> MCTensor:\n",
    "    val = torch.zeros_like(input.as_subclass(Tensor), requires_grad=requires_grad, device=device, dtype=dtype)\n",
    "    return MCTensor(val, nc=input.nc)\n",
    "\n",
    "@implements(torch.ones_like)\n",
    "def ones_like(input: MCTensor, requires_grad=False, device=None, dtype=None) -> MCTensor:\n",
    "    val = torch.ones_like(input.as_subclass(Tensor), requires_grad=requires_grad, device=device, dtype=dtype)\n",
    "    return MCTensor(val, nc=input.nc)\n",
    "\n",
    "@implements(torch.clamp)\n",
    "def clamp(input: MCTensor, min=None, max=None) -> MCTensor:\n",
    "    return _clamp(input.tensor, min=min, max=max)\n",
    "\n",
    "@implements(torch.clamp_min)\n",
    "def clamp_min(input: MCTensor, min=None) -> MCTensor:\n",
    "    return _clamp(input.tensor, min=min)\n",
    "\n",
    "@implements(torch.clamp_max)\n",
    "def clamp_max(input: MCTensor, max=None) -> MCTensor:\n",
    "    return _clamp(input.tensor, max=max)\n",
    "\n",
    "@implements(torch.norm)\n",
    "def norm(input: MCTensor, dim=None, keepdim=False, p=2, **kw) -> MCTensor:\n",
    "    return _norm(input.tensor, dim=dim, keepdim=keepdim, p=p)\n",
    "\n",
    "@implements(torch.sum)\n",
    "def sum(input: MCTensor, dim=None, keepdim=False, **kw) -> MCTensor:\n",
    "    return _sum(input.tensor, dim=dim, keepdim=keepdim)\n",
    "\n",
    "@implements(torch.abs)\n",
    "def abs(input: MCTensor) -> MCTensor:\n",
    "    result_tensor = _Renormalize(input.tensor, input.nc)\n",
    "    neg_fc_pos = result_tensor[..., 0] < 0\n",
    "    result_tensor[neg_fc_pos] = -result_tensor[neg_fc_pos]\n",
    "    return result_tensor\n",
    "\n",
    "@implements(torch.dot)\n",
    "def dot(input: Union[MCTensor, int, float], other: Union[MCTensor, int, float]) -> MCTensor:\n",
    "    if isinstance(input, MCTensor) and type(other) == Tensor:\n",
    "        x = input\n",
    "        y = other\n",
    "    elif type(input) == Tensor and isinstance(other, MCTensor):\n",
    "        x = other\n",
    "        y = input\n",
    "    elif isinstance(input, MCTensor) and isinstance(other, MCTensor):\n",
    "        return _Dot_MCN_M(input.tensor, other.tensor)\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    return _Dot_MCN(x.tensor, y)\n",
    "\n",
    "@implements(torch.mv)\n",
    "def mv(input: Union[Tensor, MCTensor], other: Union[Tensor, MCTensor]) -> MCTensor:\n",
    "    if input.dim() == 2 and other.dim() == 1:\n",
    "        x = input  # matrix\n",
    "        y = other  # vector\n",
    "    elif input.dim() == 1 and other.dim() == 2:\n",
    "        x = other  # matrix\n",
    "        y = input  # vector\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    if isinstance(x, MCTensor) and type(y) == Tensor:\n",
    "        result = _MV_MC_T(x.tensor, y)\n",
    "    elif type(x) == Tensor and isinstance(y, MCTensor):\n",
    "        result = _MV_T_MC(x, y.tensor)\n",
    "    elif isinstance(x, MCTensor) and isinstance(y, MCTensor):\n",
    "        return _MV_MC_M_M(x.tensor, y.tensor)\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    return result\n",
    "\n",
    "@implements(torch.mm)\n",
    "def mm(input: Union[Tensor, MCTensor], other: Union[Tensor, MCTensor]) -> MCTensor:\n",
    "    if isinstance(input, MCTensor) and type(other) == Tensor:\n",
    "        result = _MM_MC_T(input.tensor, other)\n",
    "    elif type(input) == Tensor and isinstance(other, MCTensor):\n",
    "        result = _MM_T_MC(input, other.tensor)\n",
    "    elif isinstance(input, MCTensor) and isinstance(other, MCTensor):\n",
    "        result = _MM_MC_MC(input.tensor, other.tensor)\n",
    "    else:\n",
    "        ## implement mm between mctensors\n",
    "        raise NotImplemented\n",
    "    return result\n",
    "\n",
    "@implements(torch.bmm)\n",
    "def bmm(input: Union[Tensor, MCTensor], other: Union[Tensor, MCTensor]) -> MCTensor:\n",
    "    if isinstance(input, MCTensor) and type(other) == Tensor:\n",
    "        result, size, nc = _BMM_MC_T(input.tensor, other)\n",
    "    elif type(input) == Tensor and isinstance(other, MCTensor):\n",
    "        result, size, nc = _BMM_T_MC(input, other.tensor)\n",
    "    elif isinstance(input, MCTensor) and isinstance(other, MCTensor):\n",
    "        result, size, nc = _BMM_MC_MC(input.tensor, other.tensor)\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    return result\n",
    "\n",
    "@implements(torch.matmul)\n",
    "def matmul(input: Union[MCTensor, Tensor], other: Union[MCTensor, Tensor]) -> MCTensor:\n",
    "    x_dim, y_dim = input.dim(), other.dim()\n",
    "    if x_dim == 1 and y_dim == 1:\n",
    "        return dot(input, other)\n",
    "    elif x_dim == 2 and y_dim == 2:\n",
    "        return mm(input, other)\n",
    "    elif (x_dim == 2 and y_dim == 1) or (x_dim == 1 and y_dim == 2):\n",
    "        return mv(input, other)\n",
    "    elif (x_dim > 2 and y_dim == 1) or (x_dim == 1 and y_dim > 2):\n",
    "        return mul(input, other)\n",
    "    elif x_dim == y_dim and x_dim == 3:\n",
    "        if isinstance(input, MCTensor) and type(other) == Tensor:\n",
    "            result, size, nc = _BMM_MC_T(input.tensor, other)\n",
    "        elif type(input) == Tensor and isinstance(other, MCTensor):\n",
    "            result, size, nc = _BMM_T_MC(input, other.tensor)\n",
    "        elif isinstance(input, MCTensor) and isinstance(other, MCTensor):\n",
    "            result, size, nc = _BMM_MC_MC(input.tensor, other.tensor)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "    elif x_dim == y_dim and x_dim == 4:\n",
    "        if isinstance(input, MCTensor) and type(other) == Tensor:\n",
    "            result, size, nc = _4DMM_MC_T(input.tensor, other)\n",
    "        elif type(input) == Tensor and isinstance(other, MCTensor):\n",
    "            result, size, nc = _4DMM_T_MC(input, other.tensor)\n",
    "        elif isinstance(input, MCTensor) and isinstance(other, MCTensor):\n",
    "            result, size, nc = _4DMM_MC_MC(input.tensor, other.tensor)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "    elif x_dim > y_dim:\n",
    "        y = other[(None,) * (x_dim - y_dim)]  # unsqueeze\n",
    "        if x_dim == 3:\n",
    "            if isinstance(input, MCTensor) and type(other) == Tensor:\n",
    "                result, size, nc = _BMM_MC_T(input.tensor, y)\n",
    "            elif type(input) == Tensor and isinstance(other, MCTensor):\n",
    "                result, size, nc = _BMM_T_MC(input, y.tensor)\n",
    "            elif isinstance(input, MCTensor) and isinstance(other, MCTensor):\n",
    "                result, size, nc = _BMM_MC_MC(input.tensor, y.tensor)\n",
    "            else:\n",
    "                raise NotImplemented\n",
    "        elif x_dim == 4:\n",
    "            if isinstance(input, MCTensor) and type(other) == Tensor:\n",
    "                result, size, nc = _4DMM_MC_T(input.tensor, y)\n",
    "            elif type(input) == Tensor and isinstance(other, MCTensor):\n",
    "                result, size, nc = _4DMM_T_MC(input, y.tensor)\n",
    "            elif isinstance(input, MCTensor) and isinstance(other, MCTensor):\n",
    "                result, size, nc = _4DMM_MC_MC(input, y.tensor)\n",
    "            else:\n",
    "                raise NotImplemented\n",
    "    elif x_dim < y_dim:\n",
    "        x = input[(None,) * (y_dim - x_dim)]  # unsqueeze\n",
    "        if y_dim == 3:\n",
    "            if isinstance(input, MCTensor) and type(other) == Tensor:\n",
    "                result, size, nc = _BMM_MC_T(x.tensor, other)\n",
    "            elif type(input) == Tensor and isinstance(other, MCTensor):\n",
    "                result, size, nc = _BMM_T_MC(x, other.tensor)\n",
    "            elif isinstance(input, MCTensor) and isinstance(other, MCTensor):\n",
    "                result, size, nc = _BMM_MC_MC(x.tensor, other.tensor)\n",
    "            else:\n",
    "                raise NotImplemented\n",
    "        elif y_dim == 4:\n",
    "            if isinstance(input, MCTensor) and type(other) == Tensor:\n",
    "                result, size, nc = _4DMM_MC_T(x.tensor, other)\n",
    "            elif type(input) == Tensor and isinstance(other, MCTensor):\n",
    "                result, size, nc = _4DMM_T_MC(x, other.tensor)\n",
    "            elif isinstance(input, MCTensor) and isinstance(other, MCTensor):\n",
    "                result, size, nc = _4DMM_MC_MC(x, other.tensor)\n",
    "            else:\n",
    "                raise NotImplemented\n",
    "    else:\n",
    "        ## implement mm between mctensors\n",
    "        raise NotImplemented\n",
    "    return result\n",
    "\n",
    "@implements(torch.addmm)\n",
    "def addmm(input: Union[MCTensor, Tensor], \n",
    "          mat1: Union[MCTensor, Tensor], \n",
    "          mat2: Union[MCTensor, Tensor], \n",
    "          beta=1.0, alpha=1.0) -> MCTensor:\n",
    "    return beta * input + alpha * (mat1 @ mat2)\n",
    "\n",
    "@implements(torch.transpose)\n",
    "def transpose(input: MCTensor, dim0, dim1) -> MCTensor:\n",
    "    d = input.dim()\n",
    "    if dim0 < 0:\n",
    "        dim0 += d\n",
    "    if dim1 < 0:\n",
    "        dim1 += d\n",
    "    val = torch.transpose(input.as_subclass(Tensor), dim0, dim1)\n",
    "    res =  torch.transpose(input.res, dim0, dim1)\n",
    "    return MCTensor.wrap_tensor_and_res_to_mctensor(val, res)\n",
    "\n",
    "@implements(torch.reshape)\n",
    "def reshape(input: MCTensor, shape) -> MCTensor:\n",
    "    data = torch.reshape(input.as_subclass(Tensor), shape)\n",
    "    extra_nc = input.res.shape[-1]\n",
    "    res = torch.reshape(input.res.view(input.res.numel() // extra_nc, extra_nc), shape + (extra_nc,))\n",
    "    return MCTensor.wrap_tensor_and_res_to_mctensor(data, res)\n",
    "\n",
    "@implements(torch.nn.functional.relu)\n",
    "def relu(input: MCTensor, inplace=False) -> MCTensor:\n",
    "    val = torch.nn.functional.relu(input.as_subclass(Tensor), inplace=inplace)\n",
    "    if inplace:\n",
    "        input.res[input.as_subclass(Tensor) == 0] = 0\n",
    "        return input\n",
    "    else:\n",
    "        res = input.res.clone()\n",
    "        res[val == 0] = 0\n",
    "        return MCTensor.wrap_tensor_and_res_to_mctensor(val, res)\n",
    "    \n",
    "@implements(torch.sigmoid)\n",
    "def sigmoid(input) -> MCTensor:\n",
    "    return 1/(torch.exp(-input) + 1)\n",
    "\n",
    "@implements(torch.nn.functional.softmax)\n",
    "def softmax(x: MCTensor, dim, *args, **kwargs) -> MCTensor:\n",
    "    return _softmax(x.tensor, dim=dim)\n",
    "\n",
    "@implements(torch.erf)\n",
    "def erf(input: MCTensor) -> MCTensor:\n",
    "    ### this is an approximation\n",
    "    ret = torch.erf(input.as_subclass(Tensor))\n",
    "    return MCTensor(ret, nc=input.nc)\n",
    "\n",
    "@implements(torch.nn.functional.dropout)\n",
    "def dropout(input: MCTensor, p=0.5, training=True, inplace=False) -> MCTensor:\n",
    "    if training:\n",
    "        val = torch.nn.functional.dropout(input.as_subclass(Tensor), p=p, training=True, inplace=inplace)\n",
    "        if inplace:\n",
    "            input.res[input.as_subclass(Tensor) == 0] = 0\n",
    "            return input\n",
    "        else:\n",
    "            res = input.res.clone()\n",
    "            res[val == 0] = 0\n",
    "            return MCTensor.wrap_tensor_and_res_to_mctensor(val, res)    \n",
    "    else:\n",
    "        return input\n",
    "    \n",
    "@implements(torch.square)\n",
    "def square(input: MCTensor) -> MCTensor:\n",
    "    return _square(input.tensor)\n",
    "\n",
    "@implements(torch.atanh)\n",
    "def atanh(input: MCTensor) -> MCTensor:\n",
    "    return _atanh(input.tensor)\n",
    "\n",
    "@implements(torch.log1p)\n",
    "def log1p(input: MCTensor) -> MCTensor:\n",
    "    return _log1p_standard(input.tensor)\n",
    "\n",
    "@implements(torch.nn.functional.linear)\n",
    "def linear(input: Union[MCTensor, Tensor], weight: Union[MCTensor, Tensor], bias=None) -> MCTensor:\n",
    "    if isinstance(input, MCTensor) and isinstance(weight, MCTensor):\n",
    "        ## attention, here make input as tensor, as mul between MCTensors not supported yet\n",
    "        input = input.as_subclass(Tensor)\n",
    "    ret = torch.matmul(input, weight.T)\n",
    "    if bias is None:\n",
    "        return ret\n",
    "    else:\n",
    "        return ret + bias\n",
    "    \n",
    "@implements(torch.diag)\n",
    "def diag(x: MCTensor, diagonal=0) -> MCTensor:\n",
    "    indices_selected = torch.arange(x.numel(), dtype=torch.int64, device=x.device).view(*x.shape)\n",
    "    selected_indices = torch.diag(indices_selected, diagonal=diagonal).view(-1)\n",
    "    val = x.as_subclass(Tensor).view(-1)[selected_indices]\n",
    "    res = x.res.view(x.numel(), x.res.shape[-1])[selected_indices]\n",
    "    return MCTensor.wrap_tensor_and_res_to_mctensor(val, res)\n",
    "\n",
    "@implements(torch.mean)\n",
    "def mean(input: MCTensor, dim=None, keepdim=False, **kw) -> MCTensor:\n",
    "    return _mean(input.tensor, dim=dim, keepdim=keepdim)\n",
    "\n",
    "@implements(torch.nn.functional.nll_loss)\n",
    "def nll_loss(input: MCTensor, target: Tensor, **kw) -> MCTensor:\n",
    "    return torch.mean(torch.diag(-input[:, target]))\n",
    "\n",
    "@implements(torch.nn.functional.log_softmax)\n",
    "def log_softmax(x: MCTensor, dim=None, **kw) -> MCTensor:\n",
    "    return _log_softmax(x.tensor, dim=dim)\n",
    "\n",
    "@implements(torch.nn.functional.cross_entropy)\n",
    "def cross_entropy(x: MCTensor, target: Tensor, reduction='mean', label_smoothing=0.0, **kw) -> MCTensor:\n",
    "    cross_entropy_x_tensor = _cross_entropy(x.tensor, target=target, reduction=reduction, label_smoothing=label_smoothing)\n",
    "    return cross_entropy_x_tensor\n",
    "\n",
    "@implements(torch.nn.functional.mse_loss)\n",
    "def mse_loss(x: MCTensor, y: MCTensor, reduction='mean', **kw) -> MCTensor:\n",
    "    return _mse_loss(x.tensor, y.tensor, reduction=reduction)\n",
    "\n",
    "@implements(torch.sqrt)\n",
    "def sqrt(input: MCTensor) -> MCTensor:\n",
    "    return _sqrt(input.tensor)\n",
    "\n",
    "@implements(torch.log)\n",
    "def log(input: MCTensor) -> MCTensor:\n",
    "    return _log(input.tensor)\n",
    "\n",
    "@implements(torch.pow)\n",
    "def pow(input: MCTensor, exponent: Union[Tensor, float, int]) -> MCTensor:\n",
    "    return  _pow(input.tensor, exponent)\n",
    "\n",
    "@implements(torch.exp)\n",
    "def exp(input: MCTensor) -> MCTensor:\n",
    "    return _exp(input.tensor)\n",
    "\n",
    "@implements(torch.sinh)\n",
    "def sinh(input: MCTensor) -> MCTensor:\n",
    "    return _sinh(input.tensor)\n",
    "\n",
    "@implements(torch.cosh)\n",
    "def cosh(input: MCTensor) -> MCTensor:\n",
    "    return _cosh(input.tensor)\n",
    "\n",
    "@implements(torch.tanh)\n",
    "def tanh(input: MCTensor) -> MCTensor:\n",
    "    return _tanh(input.tensor)\n",
    "\n",
    "@implements(torch.nn.functional.layer_norm)\n",
    "def layer_norm(x: MCTensor, normalized_shape, weight=None, bias=None, eps=1e-05) -> MCTensor:\n",
    "    nc = x.nc\n",
    "    if isinstance(weight, torch.Tensor):\n",
    "        mc_weight = torch.zeros(weight.size() + (nc,),\n",
    "                                device=x.device, dtype=x.dtype)\n",
    "        mc_weight[..., 0] = weight\n",
    "    else:\n",
    "        mc_weight = weight.tensor\n",
    "\n",
    "    if isinstance(bias, torch.Tensor):\n",
    "        mc_bias = torch.zeros(bias.size() + (nc,),\n",
    "                              device=x.device, dtype=x.dtype)\n",
    "        mc_bias[..., 0] = bias\n",
    "    else:\n",
    "        mc_bias = bias.tensor\n",
    "    return _layer_norm(x.tensor, normalized_shape, mc_weight, mc_bias, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCTensor([[0.2000, 0.2000]]), nc=2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = MCTensor([0.1, 0.1], nc=2)\n",
    "a + MCTensor([[0.1, 0.1]], nc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import HTorch.MCTensor.MCOpBasics\n",
    "# def _Grow_ExpN(x_tensor, value, simple=True):\n",
    "#     nc = x_tensor.size(-1)\n",
    "#     Q = value\n",
    "#     h = torch.zeros_like(x_tensor)\n",
    "#     for i in range(1, nc+1):\n",
    "#         Q, hval = HTorch.MCTensor.MCOpBasics.Two_Sum(x_tensor[..., -i], Q)\n",
    "#         if i == 1:\n",
    "#             last_tensor = hval.data\n",
    "#         else:\n",
    "#             print(h.shape)\n",
    "#             print(hval.shape)\n",
    "#             h[..., -(i-1)] = hval.data\n",
    "#     h[..., 0] = Q\n",
    "#     # change from .unsqueeze(-1) to (*.shape, 1)\n",
    "#     if simple:\n",
    "#         res = torch.cat([h.data, last_tensor.data.view(\n",
    "#             (*last_tensor.shape, 1))], dim=-1)\n",
    "#         # if res.dim() == 1:\n",
    "#         #     h.data.copy_(_Simple_renormalize_old(res.view(1, len(res)), r_nc=nc)[0])\n",
    "#         # else:\n",
    "#         h.data.copy_(_Simple_renormalize_old(res, r_nc=nc))\n",
    "#     else:\n",
    "#         res = torch.cat([h.data, last_tensor.data.view(\n",
    "#             (*last_tensor.shape, 1))], dim=-1)\n",
    "#         # if res.dim() == 1:\n",
    "#         #     h.data.copy_(_Renormalize(res.view(1, len(res)), r_nc=nc)[0])\n",
    "#         # else:\n",
    "#         h.data.copy_(_Renormalize(res, r_nc=nc))\n",
    "#     return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([2])\n",
      "MCTensor(0.2000), nc=2\n"
     ]
    }
   ],
   "source": [
    "a = MCTensor([0.1], nc=2)\n",
    "# print(a[0])\n",
    "# print(a[0] + a[0]) # []\n",
    "# print(a[0].tensor.shape)\n",
    "print(a[0].shape)\n",
    "print(a[0].tensor.shape)\n",
    "print(a[0] + a[0].tensor)\n",
    "\n",
    "# print(_Grow_ExpN(a[0].tensor.unsqueeze(0), a[0].tensor)[0])\n",
    "\n",
    "# print(a[0] * a[0])\n",
    "# print(a[0] * a[0].tensor)\n",
    "# print(a[0] / a[0])\n",
    "# print(a[0] / a[0].tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDos and attentions:\n",
    "1. check each (customized) function is really called and working instead of super torch class functions\n",
    "2. fix torch.abs to be correct, i.e., make it a neg for those with negative first component\n",
    "3. support torch.norm with p='fro' Frobenius norm\n",
    "4. check renormalize, i.e., case number (0,1,2) and simple (True,False) in add, mul, div, etc., specifically when they use lower level functions such as _AddMCN, _ScalingN ...\n",
    "5. implement MCTensor multiplication with MCTensor at high level, i.e., dot, mv, mm etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([2, 3, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = MCTensor([[0.1, 0.3, 0.2], [0.2, 0.4, 0.5]], nc=2)\n",
    "a.res.add_(-0.001)\n",
    "a.shape, a.res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCTensor([[0.1000, 0.3000, 0.2000],\n",
      "          [0.2000, 0.4000, 0.4999]]), nc=2\n",
      "tensor([[[-1.3206e-09],\n",
      "         [ 1.0938e-08],\n",
      "         [-2.6412e-09]],\n",
      "\n",
      "        [[-2.6412e-09],\n",
      "         [-5.2823e-09],\n",
      "         [ 8.2982e-09]]])\n"
     ]
    }
   ],
   "source": [
    "a = MCTensor([[-0.1, 0.3, 0.2], [0.2, -0.4, 0.5]], nc=2)\n",
    "a.res = (a.as_subclass(Tensor) * -1e-4).view(a.res.shape)\n",
    "print(torch.abs(a))\n",
    "print(torch.abs(a).res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCTensor([[[ 1.0000],\n",
       "           [ 3.6056],\n",
       "           [ 6.4031]],\n",
       "\n",
       "          [[ 9.2195],\n",
       "           [12.0416],\n",
       "           [14.8661]],\n",
       "\n",
       "          [[17.6918],\n",
       "           [20.5183],\n",
       "           [23.3452]],\n",
       "\n",
       "          [[26.1725],\n",
       "           [29.0000],\n",
       "           [31.8277]],\n",
       "\n",
       "          [[34.6554],\n",
       "           [37.4833],\n",
       "           [40.3113]]]), nc=2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(30).reshape(5, 3, 2).float()\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "torch.norm(mc_a, p='fro', keepdim=True, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0000],\n",
       "         [ 3.6056],\n",
       "         [ 6.4031]],\n",
       "\n",
       "        [[ 9.2195],\n",
       "         [12.0416],\n",
       "         [14.8661]],\n",
       "\n",
       "        [[17.6918],\n",
       "         [20.5183],\n",
       "         [23.3452]],\n",
       "\n",
       "        [[26.1725],\n",
       "         [29.0000],\n",
       "         [31.8277]],\n",
       "\n",
       "        [[34.6554],\n",
       "         [37.4833],\n",
       "         [40.3113]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(a, p=2, keepdim=True, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8555.)\n",
      "MCTensor(8555.), nc=2\n",
      "MCTensor(0.), nc=2\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(30).float()\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "b = torch.arange(30).float()\n",
    "mc_b = MCTensor(b, nc=2)\n",
    "\n",
    "print(torch.dot(a, b))\n",
    "print(torch.dot(mc_a, mc_b))\n",
    "print(torch.matmul(mc_a, mc_b) - torch.matmul(a, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCTensor([ 55., 145., 235., 325., 415.]), nc=2\n",
      "tensor([ 55., 145., 235., 325., 415.])\n",
      "MCTensor([0., 0., 0., 0., 0.]), nc=2\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(30).reshape(5, 6).float()\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "b = torch.arange(6).float()\n",
    "mc_b = MCTensor(b, nc=2)\n",
    "\n",
    "print(torch.mv(mc_a, mc_b))\n",
    "print(torch.mv(a, b))\n",
    "print(torch.matmul(mc_a, mc_b) - torch.matmul(a, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCTensor([[ 550.,  565.,  580.,  595.,  610.,  625.,  640.,  655.,  670.,  685.],\n",
      "          [1450., 1501., 1552., 1603., 1654., 1705., 1756., 1807., 1858., 1909.],\n",
      "          [2350., 2437., 2524., 2611., 2698., 2785., 2872., 2959., 3046., 3133.],\n",
      "          [3250., 3373., 3496., 3619., 3742., 3865., 3988., 4111., 4234., 4357.],\n",
      "          [4150., 4309., 4468., 4627., 4786., 4945., 5104., 5263., 5422., 5581.]]), nc=2\n",
      "tensor([[ 550.,  565.,  580.,  595.,  610.,  625.,  640.,  655.,  670.,  685.],\n",
      "        [1450., 1501., 1552., 1603., 1654., 1705., 1756., 1807., 1858., 1909.],\n",
      "        [2350., 2437., 2524., 2611., 2698., 2785., 2872., 2959., 3046., 3133.],\n",
      "        [3250., 3373., 3496., 3619., 3742., 3865., 3988., 4111., 4234., 4357.],\n",
      "        [4150., 4309., 4468., 4627., 4786., 4945., 5104., 5263., 5422., 5581.]])\n",
      "MCTensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), nc=2\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(30).reshape(5, 6).float()\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "b = torch.arange(60).reshape(6, 10).float()\n",
    "mc_b = MCTensor(b, nc=2)\n",
    "\n",
    "# 5, 6\n",
    "print(torch.mm(mc_a, mc_b))\n",
    "print(torch.mm(a, b))\n",
    "print(torch.matmul(mc_a, mc_b) - torch.matmul(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCTensor([[[  10.,   13.],\n",
      "           [  28.,   40.]],\n",
      "\n",
      "          [[ 172.,  193.],\n",
      "           [ 244.,  274.]],\n",
      "\n",
      "          [[ 550.,  589.],\n",
      "           [ 676.,  724.]],\n",
      "\n",
      "          [[1144., 1201.],\n",
      "           [1324., 1390.]],\n",
      "\n",
      "          [[1954., 2029.],\n",
      "           [2188., 2272.]]]), nc=2\n",
      "tensor([[[  10.,   13.],\n",
      "         [  28.,   40.]],\n",
      "\n",
      "        [[ 172.,  193.],\n",
      "         [ 244.,  274.]],\n",
      "\n",
      "        [[ 550.,  589.],\n",
      "         [ 676.,  724.]],\n",
      "\n",
      "        [[1144., 1201.],\n",
      "         [1324., 1390.]],\n",
      "\n",
      "        [[1954., 2029.],\n",
      "         [2188., 2272.]]])\n",
      "MCTensor([[[0., 0.],\n",
      "           [0., 0.]],\n",
      "\n",
      "          [[0., 0.],\n",
      "           [0., 0.]],\n",
      "\n",
      "          [[0., 0.],\n",
      "           [0., 0.]],\n",
      "\n",
      "          [[0., 0.],\n",
      "           [0., 0.]],\n",
      "\n",
      "          [[0., 0.],\n",
      "           [0., 0.]]]), nc=2\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(30).reshape(5, 2, 3).float()\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "b = torch.arange(30).reshape(5, 3, 2).float()\n",
    "mc_b = MCTensor(b, nc=2)\n",
    "\n",
    "# 5, 6\n",
    "print(torch.bmm(mc_a, mc_b))\n",
    "print(torch.bmm(a, b))\n",
    "print(torch.matmul(mc_a, mc_b) - torch.matmul(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _4DMM_MC_MC(x_tensor, y_tensor):\n",
    "    x1, x2, x3, _, nc = x_tensor.size()\n",
    "    y1, y2, _, y3, nc = y_tensor.size()\n",
    "    size = max(x1, y1), max(x2, y2), x3, y3\n",
    "    scaled = _MultMCN(x_tensor.unsqueeze(-3), y_tensor.transpose(-2, -3).unsqueeze(2))\n",
    "    tmp = scaled[..., 0, :]\n",
    "    for i in range(1, scaled.size(-2)):\n",
    "        tmp = _AddMCN(tmp, scaled[..., i, :])\n",
    "    return tmp, size, nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCTensor([[[[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]]],\n",
      "\n",
      "\n",
      "          [[[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]]],\n",
      "\n",
      "\n",
      "          [[[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]]],\n",
      "\n",
      "\n",
      "          [[[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]],\n",
      "\n",
      "           [[0., 0.],\n",
      "            [0., 0.]]]]), nc=2\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(120).reshape(4, 5, 2, 3).float()\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "b = torch.arange(120).reshape(4, 5, 3, 2).float()\n",
    "mc_b = MCTensor(b, nc=2)\n",
    "\n",
    "print(torch.matmul(mc_a, mc_b) - torch.matmul(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(120).reshape(4, 5, 2, 3).float()\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "mc_a + mc_a\n",
    "print()\n",
    "\n",
    "torch.add(mc_a, mc_a)\n",
    "print()\n",
    "\n",
    "mc_a.add(mc_a)\n",
    "print()\n",
    "\n",
    "mc_a.add_(mc_a)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(120).reshape(4, 5, 2, 3).float()\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "mc_a * mc_a\n",
    "print()\n",
    "\n",
    "torch.mul(mc_a, mc_a)\n",
    "print()\n",
    "\n",
    "mc_a.mul(mc_a)\n",
    "print()\n",
    "\n",
    "mc_a.mul_(mc_a)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(120).reshape(4, 5, 2, 3).float()\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "mc_a / mc_a\n",
    "print()\n",
    "\n",
    "torch.div(mc_a, mc_a)\n",
    "print()\n",
    "\n",
    "mc_a.div(mc_a)\n",
    "print()\n",
    "\n",
    "mc_a.div_(mc_a)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(12).reshape(4, 3).float()\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "mc_a.dot(mc_a)\n",
    "print()\n",
    "\n",
    "torch.dot(mc_a, mc_a)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(12).reshape(4, 3).float()\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "mc_a.mm(mc_a.T)\n",
    "print()\n",
    "\n",
    "torch.mm(mc_a, mc_a.T)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(24).reshape(2, 4, 3).float()\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "mc_a.bmm(mc_a.transpose(-1, -2))\n",
    "print()\n",
    "\n",
    "torch.bmm(mc_a, mc_a.transpose(-1, -2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(24).reshape(2, 4, 3).float()\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "mc_a.matmul(mc_a.transpose(-1, -2))\n",
    "print()\n",
    "\n",
    "torch.matmul(mc_a, mc_a.transpose(-1, -2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(24).reshape(2, 4, 3).float()\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "mc_a.matmul(mc_a.transpose(-1, -2))\n",
    "print()\n",
    "\n",
    "torch.matmul(mc_a, mc_a.transpose(-1, -2))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3, 2)\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "mc_a.exp()\n",
    "print()\n",
    "\n",
    "mc_a.exp_()\n",
    "print()\n",
    "\n",
    "torch.exp(mc_a)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3, 2)\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "mc_a.square()\n",
    "print()\n",
    "\n",
    "mc_a.square_()\n",
    "print()\n",
    "\n",
    "torch.square(mc_a)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 2)\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "mc_a.sinh()\n",
    "print()\n",
    "\n",
    "torch.sinh(mc_a)\n",
    "print()\n",
    "\n",
    "mc_a.sinh_()\n",
    "print()\n",
    "\n",
    "mc_a.cosh()\n",
    "print()\n",
    "\n",
    "torch.cosh(mc_a)\n",
    "print()\n",
    "\n",
    "mc_a.cosh_()\n",
    "print()\n",
    "\n",
    "mc_a.tanh()\n",
    "print()\n",
    "\n",
    "torch.tanh(mc_a)\n",
    "print()\n",
    "\n",
    "mc_a.tanh_()\n",
    "print()\n",
    "\n",
    "mc_a.atanh()\n",
    "print()\n",
    "\n",
    "torch.atanh(mc_a)\n",
    "print()\n",
    "\n",
    "mc_a.atanh_()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 2)\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "torch.reshape(mc_a, (3, 2))\n",
    "print()\n",
    "\n",
    "mc_a.reshape(3, 2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 2)\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "torch.mean(mc_a, dim=0)\n",
    "print()\n",
    "\n",
    "mc_a.mean(dim=0)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 2)\n",
    "mc_a = MCTensor(a, nc=2)\n",
    "\n",
    "torch.norm(mc_a, dim=0)\n",
    "print()\n",
    "\n",
    "mc_a.norm(dim=0)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(12).reshape(2, 3, 2)\n",
    "mc_a = MCTensor(a, nc=5)\n",
    "# print(mc_a[..., -1, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 0],\n",
       "         [1, 0, 0, 0, 0]],\n",
       "\n",
       "        [[6, 0, 0, 0, 0],\n",
       "         [7, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_a.tensor[..., 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [6, 7]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[..., 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_a.res[..., 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[..., 0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
